# ğŸ“š Epistema - Wiki Explorer App

Epistema is an advanced Wikipedia reader app developed as a Mobile Computing course project. It offers a wide range of features such as voice-based search, article recommendations, offline reading, article history, image recognition using AI, and text-to-speech capabilities â€” all wrapped in a smooth Jetpack Compose UI.

## ğŸ‘¥ Team Members

- **Paarth Goyal** (Roll No: 2022343)  
- **Tejus Madan** (Roll No: 2022540)  
- **Utkarsh Dhilliwal** (Roll No: 2022551)  
- **Vaibhav Chopra** (Roll No: 2022552)

---

## âœ¨ Features

### ğŸ” Search
- Text search using Wikipedia's REST API
- ğŸ™ï¸ **Voice search** with speech-to-text integration via microphone intent

### ğŸ“° Read Articles
- Full article display with proper text and image formatting
- **Text-to-speech** feature to read articles aloud for accessibility

### ğŸ—‚ï¸ Article of the Day
- Displays the featured Wikipedia article on the home screen

### ğŸ§  Image Recognition (Lens Mode)
- Identify objects using a **locally embedded ResNet-50 model (TensorFlow Lite)**
- Snap or upload a photo â†’ app classifies it â†’ relevant Wikipedia article displayed
- Powered by **CameraX** and **TFLite**

### ğŸŒ Articles by Location
- With location permission, fetches nearby Wikipedia articles
- Uses device GPS (sensors) for geo-aware content discovery

### ğŸ“¥ Offline Mode
- Download articles to read without internet
- Articles (text + images) stored using **Room database**
- View, manage or delete saved articles from within the app

### ğŸ•˜ History
- Automatically logs visited articles with their title and main image
- Revisit past reads easily

### âš™ï¸ Settings & Accessibility
- Change **language** (English, Hindi, Spanish, French)
- Toggle **dark/light themes**
- Adjust **font sizes** (small, medium, large)

---

## ğŸ› ï¸ Tech Stack

- **Android Studio** (Kotlin, Jetpack Compose)
- **Retrofit** for HTTP requests to Wikipedia API
- **Room** for local storage
- **CameraX** for camera access
- **TensorFlow Lite (ResNet-50)** for image classification
- **Text-to-Speech** API for accessibility
- **Jetpack Navigation** + Intents for multi-screen routing

---

## â–¶ï¸ How to Run

### ğŸ“Œ Prerequisites

- Android Studio (Electric Eel or newer)
- Android Emulator or physical Android device (API 29+ recommended)
- JDK 11 or higher
- Internet connection for first-time API testing
- Ensure `google-services.json` is added if Firebase is integrated

### ğŸš€ Steps

1. **Clone the repository**
   ```bash
   git clone https://github.com/Vaibhav-Chopra-GT/AP_Project.git
   cd AP_Project
Open in Android Studio

File â†’ Open â†’ Navigate to project directory

Build & Run

Click "Run" â–¶ï¸ or Shift+F10

Grant microphone, location, and storage permissions when prompted

Testing on Emulator/Device

Ensure emulator has Play Store support for voice features

For image recognition, testing on a real device is recommended due to camera integration

ğŸ“‚ Project Structure (Highlights)
bash
Copy
Edit
app/
â”œâ”€â”€ activities/         # Each screen is an Activity (Home, Search, ReadArticle, Settings)
â”œâ”€â”€ adapters/           # RecyclerView & list adapters
â”œâ”€â”€ api/                # Retrofit interfaces and models
â”œâ”€â”€ db/                 # Room entities and DAOs for offline storage
â”œâ”€â”€ tflite/             # Image recognition model (resnet50.tflite)
â”œâ”€â”€ ui/                 # Jetpack Compose UI components
â”œâ”€â”€ utils/              # Helper classes, TTS, Location, Theme Utils
â””â”€â”€ resources/          # Drawable, layouts, strings, themes
